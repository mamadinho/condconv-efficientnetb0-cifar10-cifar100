# Final Project for CS7643 Deep Learning @ Georgia Tech

This research looks at the profile of Conditionally Parameterized Convolutions (CondConv) within the EfficientNet-B0 framework in terms of performance on image classification tasks on CIFAR-10 and CIFAR-100.
We analyzed whether dynamic convolutions, helped by sophisticated data augmentation schemes like MixUp and AutoAugment, yield performance benefits that rivalcharacteristics of conventional model scaling with expensive additional costs.
Experiments show that CondConv itself has meaningful improvements from baseline performance and a degree further improvements through consideration with MixUp and AutoAugment, pointing to combinations of such methods with strong data augmentation strategies.
Smaller dataset in CIFAR-10 and CIFAR-100 is used to see the efficiency of this approach for smaller than ImageNet dataset. 

## Results:
![image](https://github.com/user-attachments/assets/030bf857-02cd-4d3f-b39d-3ea2763c3417)

Paper Writeup: [Overleaf](https://www.overleaf.com/read/nnrhdwgbcgcw#29a002)
